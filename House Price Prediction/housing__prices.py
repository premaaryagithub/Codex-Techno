# -*- coding: utf-8 -*-
"""Housing _prices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UmN8UKAbXSXq7l88fhqcgzIhyyg4uoEw
"""

# uploaded dataset in Google Drive , Extracting  It from Drive itself
file_id  = '1pxjMjtOrsmHJ7IT-AtCET2xABg2I56Iz'
download_url = f'https://drive.google.com/uc?id={file_id}&export=download'

# importing  pandas for data manipulation
import pandas as pd
data = pd.read_csv(download_url, encoding='latin-1')

#data display  for first five rows


data.head()

#check for null values and duplicates values  and statistics and also total info of the dataset
display(data.shape)

data.isnull().sum()

data.drop_duplicates

#after dropping duplicates shape of dataset
display(data.shape)

display(data.describe())

display(data.info())

data.dropna()

# change binary columns to numerical for model training ...
data_conversion = ['mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea']
mapping = {'yes':1,'no':0}

for col in data_conversion :
    data[col] = data[col].map(mapping)

data_conversion1 = ['furnishingstatus']
mapping1 = {'furnished':2,'semi-furnished':1,'unfurnished':0}

for col in data_conversion1 :
    data[col] = data[col].map(mapping1)

display(data)

#from above dataset we can say that price was the target variable... as we all know that price of the house is mostly depend upon all other features...
import matplotlib.pyplot as plt
import seaborn as sns

# general correaltion heatmap for .. measuring dependancy ..
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# from above price was depend upon all other features .. so we should select features other than target varaibale for furthuer model  training
X = data.drop('price', axis=1)
Y = data['price']

# split the dataset into training and testing dataset .. we will take 75 25 share  as traintest split
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)

#split completed now model introducing .. in this we are training linear regression model : it should explain how the relation between dependent and independent varaibles ..
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, Y_train)

# model fitting  or model training completed now.. time to predict the  outcome . for above ...
Y_pred = model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

MSE = mean_squared_error(Y_test, Y_pred)
RMSE = np.sqrt(MSE)
MAPE = np.mean(np.abs((Y_test - Y_pred) / Y_test)) * 100
r2 = r2_score(Y_test, Y_pred)

display("\nModel Performance:")
display(f"Mean Squared Error: {MSE}")
display(f"Root Mean Squared Error: {RMSE}")
display(f"Mean Absolute Percentage Error: {MAPE}%")
display(f"R-squared Score: {r2}")
display(f'accuracy : {r2 * 100 }')

# visual insights from abvoe
# Actual vs Predicted values plot
plt.figure(figsize=(10, 6))
plt.scatter(Y_test, Y_pred, alpha=0.5)
plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Housing Prices')
plt.show()

# Residual plot
residuals = Y_test - Y_pred
plt.figure(figsize=(10, 6))
plt.scatter(Y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='-')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

# Feature importance (coefficients)
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
}).sort_values(by='Coefficient', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Feature', data=coefficients)
plt.title('Feature Importance (Coefficients)')
plt.show()

